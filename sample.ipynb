{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "To debug a cell, press Alt+Shift+Enter, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize Spark session\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"DataProcessingApp\")\n",
    "         .config(\"spark.executor.memory\", \"12g\")\n",
    "         .config(\"spark.driver.memory\", \"12g\")\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv('data/2019-2020_school_year/pdets.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop('content_source', 'tutoring_types')\n",
    "\n",
    "# Describe the DataFrame and format the output\n",
    "df.describe().show()"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Filter rows where 'problem_id' is not null\n",
    "df = df.filter(df[\"problem_id\"].isNotNull()).filter(df.skills.isNotNull())\n",
    "\n",
    "# Convert 'problem_id' to integer type\n",
    "df = df.withColumn(\"problem_id\", col(\"problem_id\").cast(IntegerType()))\n",
    "\n",
    "# Show the first few rows\n",
    "df.show(10)"
   ],
   "id": "c31141d3b1f55b01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import isnan, when, count\n",
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ],
   "id": "a518e3c1bd45c510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ast import literal_eval\n",
    "from pyspark.sql.functions import col, lit, when, explode, split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Fill null values in 'skills' column with empty list\n",
    "df = df.withColumn(\"skills\", when(col(\"skills\").isNull(), \"[]\").otherwise(col(\"skills\")))\n",
    "print(df.count(), '....')\n",
    "\n",
    "# Define a UDF to split skills and replicate rows\n",
    "def process_skills(skills):\n",
    "    if skills == '[]':\n",
    "        return [(None, None, None)]  # Return an empty list to avoid None values in explode\n",
    "    else:\n",
    "        return [(skill.split('.')[0], skill.split('.')[1], skill.split('.')[2]) for skill in literal_eval(skills)]\n",
    "\n",
    "process_skills_udf = F.udf(process_skills, ArrayType(ArrayType(StringType())))\n",
    "\n",
    "# Apply the UDF and explode the resulting array\n",
    "df = df.withColumn(\"skills_array\", process_skills_udf(col(\"skills\")))\n",
    "df = df.withColumn(\"skills_exploded\", explode(col(\"skills_array\")))\n",
    "\n",
    "# Select and rename columns\n",
    "new_df = df.select(\n",
    "    col(\"problem_id\"),\n",
    "    col(\"skills\"),\n",
    "    col(\"problem_type\"),\n",
    "    col(\"student_answer_count\"),\n",
    "    col(\"mean_correct\"),\n",
    "    col(\"mean_time_on_task\"),\n",
    "    col(\"skills_exploded\").getItem(0).alias(\"grade\"),\n",
    "    col(\"skills_exploded\").getItem(1).alias(\"domain\"),\n",
    "    col(\"skills_exploded\").getItem(2).alias(\"subdomain\")\n",
    ")\n",
    "\n",
    "# Show the first few rows\n",
    "new_df.show()"
   ],
   "id": "9c2169f3228094ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.count()",
   "id": "418a77a62a671fb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One problem can belong to maximum 4 unique classes",
   "id": "e7e9754abae5a660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plogs = spark.read.csv('data/2019-2020_school_year/plogs.csv', header=True, inferSchema=True)\n",
    "print(plogs.count())\n",
    "plogs = plogs.select('assignment_id', 'problem_id').distinct()\n",
    "plogs.show(10)"
   ],
   "id": "948a290ee9904fee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print('Debugging start')\n",
    "new_df_no_skills = new_df.where(new_df.subdomain.isNull())\n",
    "\n",
    "# Step 1: Find all `assignment_id`s associated with each `problem_id` in `new_df_no_skills`.\n",
    "plogs_problem = new_df_no_skills.join(plogs, on='problem_id', how='left')\n",
    "\n",
    "# Step 2: For each `assignment_id` found, get related rows in `plogs`\n",
    "plogs_assignment_related = plogs.join(plogs_problem, on='assignment_id', how='inner') \\\n",
    "    .select('assignment_id', 'problem_id', 'grade', 'domain', 'subdomain')\n",
    "\n",
    "# Step 3: Calculate the most common `grade`, `domain`, and `subdomain` for each `problem_id`.\n",
    "window_spec = Window.partitionBy('problem_id')\n",
    "\n",
    "# Using `F.first` on mode-sorted columns to get the mode (most common) value\n",
    "assignment_problems = plogs_assignment_related \\\n",
    "    .withColumn('mode_grade', F.first('grade').over(window_spec.orderBy(F.col('grade').desc()))) \\\n",
    "    .withColumn('mode_domain', F.first('domain').over(window_spec.orderBy(F.col('domain').desc()))) \\\n",
    "    .withColumn('mode_subdomain', F.first('subdomain').over(window_spec.orderBy(F.col('subdomain').desc()))) \\\n",
    "    .select('problem_id', 'mode_grade', 'mode_domain', 'mode_subdomain') \\\n",
    "    .distinct()\n",
    "\n",
    "# Step 4: Join the mode calculations back to `new_df` to update columns\n",
    "new_df = new_df.join(\n",
    "    assignment_problems,\n",
    "    on='problem_id',\n",
    "    how='left'\n",
    ").withColumn(\n",
    "    'grade', F.coalesce(new_df['grade'], assignment_problems['mode_grade'])\n",
    ").withColumn(\n",
    "    'domain', F.coalesce(new_df['domain'], assignment_problems['mode_domain'])\n",
    ").withColumn(\n",
    "    'subdomain', F.coalesce(new_df['subdomain'], assignment_problems['mode_subdomain'])\n",
    ")\n",
    "new_df.show(10)"
   ],
   "id": "b702e66a33f6a35d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assignment_problems = None\n",
    "for (idx, row) in new_df.iterrows():\n",
    "    if row['skills'] == '[]':\n",
    "        plogs_idx = plogs[plogs['problem_id'] == row['problem_id']]\n",
    "        related_assignments = plogs_idx['assignment_id'].unique()\n",
    "        plogs_assignment = plogs[plogs['assignment_id'].isin(related_assignments)]\n",
    "        # all plogs related to all assignments which related to the problem\n",
    "        assignment_problems = pd.merge(plogs_assignment, new_df, 'left', on='problem_id')\n",
    "\n",
    "        try:\n",
    "            # Ensure mode() result is not empty before accessing its first element\n",
    "            impute_grade = assignment_problems['grade'].mode()\n",
    "            impute_domain = assignment_problems['domain'].mode()\n",
    "            impute_subdomain = assignment_problems['subdomain'].mode()\n",
    "\n",
    "            if not impute_grade.empty:\n",
    "                impute_grade = impute_grade[0]\n",
    "            else:\n",
    "                impute_grade = None  # or some other default value\n",
    "\n",
    "            if not impute_domain.empty:\n",
    "                impute_domain = impute_domain[0]\n",
    "            else:\n",
    "                impute_domain = None\n",
    "\n",
    "            if not impute_subdomain.empty:\n",
    "                impute_subdomain = impute_subdomain[0]\n",
    "            else:\n",
    "                impute_subdomain = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {row['problem_id']}: {e}\")\n",
    "            continue\n",
    "\n",
    "        row_to_update = new_df[new_df['problem_id'] == row['problem_id']]\n",
    "        new_df.loc[new_df['problem_id'] == row['problem_id'], ['grade', 'domain', 'subdomain']] = [impute_grade, impute_domain, impute_subdomain]\n",
    "\n",
    "new_df.head(10000)"
   ],
   "id": "3d241f3b8d581c46",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
